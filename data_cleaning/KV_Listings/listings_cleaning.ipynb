{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca666021",
   "metadata": {},
   "source": [
    "Rental data cleaning and exploration for the listings scraped in Tartu and Tallinn\n",
    "In this notebook we look over the scraped data from kv.ee and discard any unimportant, unhelpful or unneccesary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f7a36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#Display settings for better data viewing\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "\n",
    "data_folder = Path('../../Data_11.04.25')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e3375",
   "metadata": {},
   "source": [
    "Listing all the files in our specific data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fec9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_files = {\n",
    "    'tallinn': data_folder / 'scraped_listings_tln.json',\n",
    "    'tartu': data_folder / 'scraped_listings_trt.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7b2bc",
   "metadata": {},
   "source": [
    "Creating cleaning functions for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e6475",
   "metadata": {},
   "source": [
    "First extracting price from format price / area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db7b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_price(price_str):\n",
    "    \"\"\"Extract monthly rent from '450 € 6.79 €/m²' format\"\"\"\n",
    "    if pd.isna(price_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        clean_str = str(price_str).replace(' ', '').split('€')[0]\n",
    "        return float(clean_str)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d936e",
   "metadata": {},
   "source": [
    "Extracting area from similar format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9534481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sqm(area_str):\n",
    "    \"\"\"Extract numeric area from '66.3 m²' format\"\"\"\n",
    "    if pd.isna(area_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        clean_str = str(area_str).replace(' ', '').replace('m²', '')\n",
    "        return float(clean_str)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac93fd",
   "metadata": {},
   "source": [
    "Extracting the floor / floors from formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9079d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_floor(floor_str):\n",
    "    \"\"\"Extract floor and total floors from '3/9' format\"\"\"\n",
    "    if pd.isna(floor_str):\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        parts = floor_str.split('/')\n",
    "        floor = int(parts[0].strip())\n",
    "        total = int(parts[1].strip())\n",
    "        return floor, total\n",
    "    except:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41806160",
   "metadata": {},
   "source": [
    "Cleaning each city data with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city_data(file_path, city_name):\n",
    "    \"\"\"Clean data for a single city\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROCESSING: {city_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Loading data from json\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            print(f\"✓ Loaded {len(data)} records from {file_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {file_path.name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Converting to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    print(f\"Initial columns: {list(df.columns)}\\n\")\n",
    "    \n",
    "    # Stripping whitespace and replacing \"?\" with NaN\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].str.strip()\n",
    "            df[column] = df[column].replace(['?', ' ?', '? '], np.nan)\n",
    "            df[column] = df[column].str.replace('\\xa0', ' ', regex=False)\n",
    "    \n",
    "    # Extracting clean numeric values using our functions\n",
    "    df['price_clean'] = df['price'].apply(extract_price)\n",
    "    df['area_sqm'] = df['Üldpind'].apply(extract_sqm)\n",
    "    df[['floor', 'total_floors']] = df['Korrus/Korruseid'].apply(\n",
    "        lambda x: pd.Series(extract_floor(x))\n",
    "    )\n",
    "    df['rooms'] = pd.to_numeric(df['Tube'], errors='coerce')\n",
    "    \n",
    "    # Cleaning the build year (no values under 1800 or over 2025)\n",
    "    df['build_year'] = pd.to_numeric(df['Ehitusaasta'], errors='coerce')\n",
    "    df.loc[df['build_year'] < 1800, 'build_year'] = np.nan\n",
    "    df.loc[df['build_year'] > 2025, 'build_year'] = np.nan\n",
    "    \n",
    "    # Validating coordinates - going to numeric\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "    \n",
    "    # COLUMN SELECTION BASED ON MISSING DATA THRESHOLD\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"COLUMN EVALUATION - Missing Data Analysis (Threshold: 20%)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    missing_threshold = 20  # Removing columns with >20% missing data\n",
    "    \n",
    "    # Define columns to exclude or always keep\n",
    "    exclude_cols = [\n",
    "        'price', 'Üldpind', 'Korrus/Korruseid', 'Ehitusaasta', 'Tube'\n",
    "    ]\n",
    "    essential_cols = [\n",
    "        'id', 'url', 'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Evaluating all columns\n",
    "    columns_to_keep = []\n",
    "    columns_removed = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        \n",
    "        # Skip if this is a raw column we're replacing with cleaned version\n",
    "        if col in exclude_cols:\n",
    "            columns_removed.append(col)\n",
    "            print(f\"✗ REMOVE {col:30s} | {missing_pct:6.2f}% missing | [REPLACED BY CLEANED VERSION]\")\n",
    "        elif col in essential_cols:\n",
    "            columns_to_keep.append(col)\n",
    "            print(f\"✓ KEEP   {col:30s} | {missing_pct:6.2f}% missing | [ESSENTIAL]\")\n",
    "        elif missing_pct <= missing_threshold:\n",
    "            columns_to_keep.append(col)\n",
    "            print(f\"✓ KEEP   {col:30s} | {missing_pct:6.2f}% missing\")\n",
    "        else:\n",
    "            columns_removed.append(col)\n",
    "            print(f\"✗ REMOVE {col:30s} | {missing_pct:6.2f}% missing | [EXCEEDS THRESHOLD]\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SUMMARY: Kept {len(columns_to_keep)} columns, Removed {len(columns_removed)} columns\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if columns_removed:\n",
    "        print(f\"\\nRemoved columns: {', '.join(columns_removed)}\")\n",
    "    \n",
    "    # Create filtered dataframe\n",
    "    df_filtered = df[columns_to_keep].copy()\n",
    "    \n",
    "    # Map to clean English names\n",
    "    column_mapping = {\n",
    "        'id': 'id',\n",
    "        'url': 'url',\n",
    "        'price': 'price_raw',\n",
    "        'latitude': 'latitude',\n",
    "        'longitude': 'longitude',\n",
    "        'Üürida korter': 'rental_ad',\n",
    "        'Tube': 'rooms_raw',\n",
    "        'Üldpind': 'area_raw',\n",
    "        'Korrus/Korruseid': 'floor_raw',\n",
    "        'Ehitusaasta': 'build_year_raw',\n",
    "        'Seisukord': 'condition',\n",
    "        'Korruseid': 'total_floors_alt',\n",
    "        'Magamistube': 'bedrooms',\n",
    "        'Energiamärgis': 'energy_label',\n",
    "        'Omandivorm': 'ownership_type',\n",
    "        'price_clean': 'price',\n",
    "        'area_sqm': 'area_sqm',\n",
    "        'rooms': 'rooms',\n",
    "        'floor': 'floor',\n",
    "        'total_floors': 'total_floors',\n",
    "        'build_year': 'build_year'\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    new_column_names = []\n",
    "    for col in df_filtered.columns:\n",
    "        new_name = column_mapping.get(col, col)\n",
    "        new_column_names.append(new_name)\n",
    "    \n",
    "    df_filtered.columns = new_column_names\n",
    "    \n",
    "    # If we have both price_raw and price, drop price_raw (keep the cleaned version)\n",
    "    if 'price' in df_filtered.columns and 'price_raw' in df_filtered.columns:\n",
    "        df_filtered = df_filtered.drop(columns=['price_raw'])\n",
    "        print(\"\\n✓ Dropped 'price_raw' - keeping cleaned 'price' column\")\n",
    "    \n",
    "    # Drop rental_ad column if it exists (not needed)\n",
    "    if 'rental_ad' in df_filtered.columns:\n",
    "        df_filtered = df_filtered.drop(columns=['rental_ad'])\n",
    "        print(\"✓ Dropped 'rental_ad' - not needed for analysis\")\n",
    "    \n",
    "    # Same for other duplicate raw columns\n",
    "    duplicate_raw_cols = [col for col in df_filtered.columns if col.endswith('_raw')]\n",
    "    if duplicate_raw_cols:\n",
    "        # Only drop raw columns if we have their cleaned versions\n",
    "        to_drop = []\n",
    "        for raw_col in duplicate_raw_cols:\n",
    "            clean_col = raw_col.replace('_raw', '')\n",
    "            if clean_col in df_filtered.columns:\n",
    "                to_drop.append(raw_col)\n",
    "        if to_drop:\n",
    "            df_filtered = df_filtered.drop(columns=to_drop)\n",
    "            print(f\"✓ Dropped raw columns: {', '.join(to_drop)}\")\n",
    "    \n",
    "    print(f\"\\nFinal columns after cleanup:\")\n",
    "    print(f\"{list(df_filtered.columns)}\\n\")\n",
    "    \n",
    "    # DATA QUALITY CHECKS\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"DATA QUALITY CHECKS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Removing records missing critical fields\n",
    "    critical_fields = ['price', 'area_sqm', 'rooms']\n",
    "    before = len(df_filtered)\n",
    "    df_final = df_filtered.dropna(subset=critical_fields)\n",
    "    removed_missing = before - len(df_final)\n",
    "    print(f\"\\n✓ Removed {removed_missing} records missing critical fields (price, area, rooms)\")\n",
    "    \n",
    "    # Remove unrealistic prices\n",
    "    before = len(df_final)\n",
    "    df_final = df_final[df_final['price'] >= 50]\n",
    "    removed_price = before - len(df_final)\n",
    "    print(f\"✓ Removed {removed_price} records with price < 50€ (likely errors)\")\n",
    "    \n",
    "    # Price distribution\n",
    "    print(f\"\\nPrice distribution:\")\n",
    "    print(f\"  50-100€:    {((df_final['price'] >= 50) & (df_final['price'] < 100)).sum():4d} listings\")\n",
    "    print(f\"  100-500€:   {((df_final['price'] >= 100) & (df_final['price'] < 500)).sum():4d} listings\")\n",
    "    print(f\"  500-1000€:  {((df_final['price'] >= 500) & (df_final['price'] < 1000)).sum():4d} listings\")\n",
    "    print(f\"  > 1000€:    {(df_final['price'] >= 1000).sum():4d} listings\")\n",
    "    \n",
    "    # Area distribution\n",
    "    print(f\"\\nArea distribution:\")\n",
    "    print(f\"  < 15 m²:    {(df_final['area_sqm'] < 15).sum():4d} listings\")\n",
    "    print(f\"  15-50 m²:   {((df_final['area_sqm'] >= 15) & (df_final['area_sqm'] < 50)).sum():4d} listings\")\n",
    "    print(f\"  50-100 m²:  {((df_final['area_sqm'] >= 50) & (df_final['area_sqm'] < 100)).sum():4d} listings\")\n",
    "    print(f\"  > 100 m²:   {(df_final['area_sqm'] >= 100).sum():4d} listings\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FINAL {city_name.upper()} DATASET: {len(df_final)} rows × {len(df_final.columns)} columns\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nMissing values in final dataset:\")\n",
    "    missing_summary = df_final.isnull().sum()\n",
    "    missing_summary = missing_summary[missing_summary > 0]\n",
    "    if len(missing_summary) > 0:\n",
    "        for col, count in missing_summary.items():\n",
    "            pct = (count / len(df_final)) * 100\n",
    "            print(f\"  {col:20s}: {count:4d} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  No missing values in critical columns!\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570a357",
   "metadata": {},
   "source": [
    "Saving cleaned dataframes to new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bec3223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RENTAL DATA CLEANING - TARTU & TALLINN\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: TALLINN\n",
      "======================================================================\n",
      "✓ Loaded 1359 records from scraped_listings_tln.json\n",
      "Initial DataFrame shape: (1359, 20)\n",
      "Initial columns: ['id', 'url', 'price', 'latitude', 'longitude', 'Üürida korter', 'Tube', 'Üldpind', 'Korrus/Korruseid', 'Ehitusaasta', 'Seisukord', 'Korruseid', 'Magamistube', 'Energiamärgis', 'Omandivorm', 'Ettemaks', 'Kulud suvel/talvel', 'Katastrinumber', 'Üürida korter (Broneeritud)', 'Registriosa number']\n",
      "\n",
      "======================================================================\n",
      "COLUMN EVALUATION - Missing Data Analysis (Threshold: 20%)\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exclude_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m results = {}\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m city_name, file_path \u001b[38;5;129;01min\u001b[39;00m city_files.items():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df_city = \u001b[43mclean_city_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcity_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df_city \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     14\u001b[39m         output_path = output_folder / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlistings_cleaned_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mclean_city_data\u001b[39m\u001b[34m(file_path, city_name)\u001b[39m\n\u001b[32m     58\u001b[39m missing_pct = (missing_count / \u001b[38;5;28mlen\u001b[39m(df)) * \u001b[32m100\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Skip if this is a raw column we're replacing with cleaned version\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexclude_cols\u001b[49m:\n\u001b[32m     62\u001b[39m     columns_removed.append(col)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✗ REMOVE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m30s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_pct\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% missing | [REPLACED BY CLEANED VERSION]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'exclude_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Process each city\n",
    "output_folder = Path('../../Cleaned_csvs')\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RENTAL DATA CLEANING - TARTU & TALLINN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {}\n",
    "for city_name, file_path in city_files.items():\n",
    "    df_city = clean_city_data(file_path, city_name)\n",
    "    \n",
    "    if df_city is not None:\n",
    "        output_path = output_folder / f'listings_cleaned_{city_name}.csv'\n",
    "        df_city.to_csv(output_path, index=False, encoding='utf-8', sep=\";\")\n",
    "        print(f\"\\n✓ Saved to: {output_path}\")\n",
    "        results[city_name] = len(df_city)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "for city, count in results.items():\n",
    "    print(f\"  {city.capitalize():10s}: {count:4d} listings\")\n",
    "print(f\"  {'Total':10s}: {sum(results.values()):4d} listings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
